# Adversarially Robust Policy Learning through Active construction of Physically-plausible Perturbations

<iframe width="560" height="315" src="https://www.youtube.com/embed/L561cJh7DLE?autoplay=0&showinfo=0&controls=2&modestbranding=1&rel=0&theme=light" frameborder="10" allowfullscreen></iframe>
\[Placeholder video -- deadline Mar 5\]

### Abstract
Policy learning in reinforcement learning has demonstrated success in scaling up problem size beyond toy examples. However, enabling these methods on real robots poses a challenge of both sample complexity during learning and safety against malicious intervention. Model-based method using simulated approximations of the target domains offer a possible solution, with the caveat that algorithms need to adapt across errors in modelling and adversarial perturbations. We introduce Adversarially Robust Policy Learning (ARPL) that leverage active construction of physically-plausible adversarial examples during training to enable sample-effiecient policy learning in source and resulting in a robust policy that performs well under both random perturbations as well as adversarial input manipulations. We further show that ARPL improves distance to uncontrollability in simplified environments, hence providing a justification for improved robustness in more complex environments. We evaluate our method on four tasks with continuous control and show superior performance of ARPL as compared to state-of-the-art robust policy learning methods.

### Methods
\[Put a short blurb here\]

### Results
\[Additional results and supplments go here\]

### References
- [**Adversarially Robust Policy Learning through Active construction of Physically-plausible Perturbations**]().  
Ajay Mandlekar\*, Yuke Zhu\*, Animesh Garg, Li Fei-Fei, Silvio Savarese (\* denotes equal contribution).  
*Under review at IEEE International Conference on Intelligent Robotics and Systems, (IROS) 2017*

### Authors and Contributors  

Ajay Mandlekar, [Yuke Zhu](https://web.stanford.edu/~yukez/), [Animesh Garg](http://ai.stanford.edu/~garg/)  
PIs: [Fei-Fei Li](http://vision.stanford.edu/feifeili/), [Silvio Savarese](cvgl.stanford.edu/silvio/)

### Support or Contact

Please Contact [Animesh Garg](http://ai.stanford.edu/~garg/) at [garg@cs.stanford.edu](mail:garg@cs.stanford.edu)
